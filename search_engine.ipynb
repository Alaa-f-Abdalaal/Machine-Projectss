{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM5GeLm22M3StaS6XWBx3n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alaa-f-Abdalaal/Machine-Projectss/blob/main/search_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRxMUEufstJr",
        "outputId": "cc7b7abe-bd7b-4c5f-f28f-8c80ae54556f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTN0XmjswR9m",
        "outputId": "36c42a54-d8f8-4994-bb89-c308a26acc89"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PpuU8JYPwR_-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and t.isalpha()]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "iiVKZ1JxwSCW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ 2. Load Gutenberg Books ------------------------------\n",
        "book_links = [\n",
        "    \"https://www.gutenberg.org/files/1342/1342-0.txt\",  # Pride and Prejudice\n",
        "    \"https://www.gutenberg.org/files/11/11-0.txt\",      # Alice in Wonderland\n",
        "    \"https://www.gutenberg.org/files/98/98-0.txt\"       # A Tale of Two Cities\n",
        "]\n",
        "\n",
        "def load_gutenberg_book(url):\n",
        "    r = requests.get(url)\n",
        "    r.encoding = \"utf-8\"\n",
        "    return r.text\n",
        "\n",
        "def clean_gutenberg_text(text):\n",
        "    start = \"*** START OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "    end = \"*** END OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "    if start in text and end in text:\n",
        "        text = text.split(start)[1].split(end)[0]\n",
        "    return text\n",
        "\n",
        "docs = []\n",
        "for link in book_links:\n",
        "    raw_text = load_gutenberg_book(link)\n",
        "    clean_text = clean_gutenberg_text(raw_text)\n",
        "    docs.append(clean_text)"
      ],
      "metadata": {
        "id": "JSdECbrzwSEv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ 3. Build Positional Index ------------------------------\n",
        "def build_index(docs):\n",
        "    index = {}  # term -> {doc_id: [positions]}\n",
        "    for doc_id, doc in enumerate(docs):\n",
        "        tokens = preprocess(doc)\n",
        "        for pos, token in enumerate(tokens):\n",
        "            if token not in index:\n",
        "                index[token] = {}\n",
        "            if doc_id not in index[token]:\n",
        "                index[token][doc_id] = []\n",
        "            index[token][doc_id].append(pos)\n",
        "    return index\n",
        "\n",
        "index = build_index(docs)\n",
        "print(\"Index built for Gutenberg books!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6avR6g7wSG-",
        "outputId": "13a15de6-443f-4e82-ef09-d0a7605bf9a6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index built for Gutenberg books!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ 4. Boolean + Phrase Search ------------------------------\n",
        "def boolean_search(query, index):\n",
        "    q_tokens = preprocess(query)\n",
        "    if not q_tokens:\n",
        "        return []\n",
        "\n",
        "    if q_tokens[0] not in index:\n",
        "        return []\n",
        "    result_docs = set(index[q_tokens[0]].keys())\n",
        "    for term in q_tokens[1:]:\n",
        "        if term not in index:\n",
        "            return []\n",
        "        result_docs &= set(index[term].keys())\n",
        "\n",
        "    # Phrase check\n",
        "    if len(q_tokens) > 1:\n",
        "        valid_docs = []\n",
        "        for doc_id in result_docs:\n",
        "            pos_lists = [sorted(index[t][doc_id]) for t in q_tokens]\n",
        "            for i in range(len(pos_lists[0])):\n",
        "                if all(pos_lists[j][0] + j == pos_lists[0][i] + j for j in range(1, len(pos_lists)) if i + j < len(pos_lists[j])):\n",
        "                    valid_docs.append(doc_id)\n",
        "                    break\n",
        "        result_docs = valid_docs\n",
        "\n",
        "    return list(result_docs)"
      ],
      "metadata": {
        "id": "vkiFiIlLwSJF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ 5. Edit Distance Search ------------------------------\n",
        "def edit_distance(s1, s2):\n",
        "    m, n = len(s1), len(s2)\n",
        "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
        "    for i in range(m+1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n+1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1,m+1):\n",
        "        for j in range(1,n+1):\n",
        "            if s1[i-1]==s2[j-1]:\n",
        "                dp[i][j] = dp[i-1][j-1]\n",
        "            else:\n",
        "                dp[i][j] = 1 + min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1])\n",
        "    return dp[m][n]\n",
        "\n",
        "def fuzzy_search(query, index, max_dist=2):\n",
        "    q_tokens = preprocess(query)\n",
        "    matched_docs = set()\n",
        "    for term in index.keys():\n",
        "        for qt in q_tokens:\n",
        "            if edit_distance(term, qt) <= max_dist:\n",
        "                matched_docs.update(index[term].keys())\n",
        "    return list(matched_docs)\n"
      ],
      "metadata": {
        "id": "kAfgKaZNwSLf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ 6. Soundex Search ------------------------------\n",
        "def soundex(word):\n",
        "    word = word.upper()\n",
        "    codes = {\"B\":\"1\",\"F\":\"1\",\"P\":\"1\",\"V\":\"1\",\n",
        "             \"C\":\"2\",\"G\":\"2\",\"J\":\"2\",\"K\":\"2\",\"Q\":\"2\",\"S\":\"2\",\"X\":\"2\",\"Z\":\"2\",\n",
        "             \"D\":\"3\",\"T\":\"3\",\n",
        "             \"L\":\"4\",\n",
        "             \"M\":\"5\",\"N\":\"5\",\n",
        "             \"R\":\"6\"}\n",
        "    sound = word[0]\n",
        "    for char in word[1:]:\n",
        "        code = codes.get(char,\"0\")\n",
        "        if code != sound[-1]:\n",
        "            sound += code\n",
        "    sound = sound.replace(\"0\",\"\")\n",
        "    return (sound+\"000\")[:4]\n",
        "\n",
        "def soundex_search(query, index):\n",
        "    q_tokens = preprocess(query)\n",
        "    matched_docs = set()\n",
        "    query_sdx = [soundex(q) for q in q_tokens]\n",
        "    for term in index.keys():\n",
        "        if soundex(term) in query_sdx:\n",
        "            matched_docs.update(index[term].keys())\n",
        "    return list(matched_docs)"
      ],
      "metadata": {
        "id": "Qu6pKNgbwSOO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ 7. TF-IDF Ranking ------------------------------\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(docs)\n",
        "\n",
        "def rank_documents(query, docs_list, doc_ids):\n",
        "    if not doc_ids:\n",
        "        return []\n",
        "\n",
        "    query_vec = np.zeros((1, len(vectorizer.get_feature_names_out())))\n",
        "    q_tokens = preprocess(query)\n",
        "    for token in q_tokens:\n",
        "        if token in vectorizer.vocabulary_:\n",
        "            idx = vectorizer.vocabulary_[token]\n",
        "            query_vec[0, idx] = 1\n",
        "\n",
        "    tfidf_dense = tfidf_matrix.toarray()\n",
        "    scores = np.dot(tfidf_dense, query_vec.T).flatten()\n",
        "\n",
        "    ranked_idx = np.argsort(scores)[::-1]\n",
        "    ranked_docs = [(i, docs_list[i], scores[i]) for i in ranked_idx if i in doc_ids and scores[i] > 0]\n",
        "    return ranked_docs"
      ],
      "metadata": {
        "id": "IcpfMZK3wSRm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------ 8. Interactive TF-IDF Search ------------------------------\n",
        "while True:\n",
        "    query = input(\"\\nSearch: \").strip()\n",
        "    if query.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    # TF-IDF search (ranked)\n",
        "    doc_ids = list(range(len(docs)))  # كل الـ documents\n",
        "    ranked_results = rank_documents(query, docs, doc_ids)\n",
        "\n",
        "    print(f\"\\nFound {len(ranked_results)} document(s) (ranked by TF-IDF):\")\n",
        "    for doc_id, _, score in ranked_results[:5]:\n",
        "        print(f\"  [{doc_id}] Score: {score:.4f}, Link: {book_links[doc_id]})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5UIVVNn1GuS",
        "outputId": "61e6e198-8207-4c16-e8a2-4dfc6d2a26e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Search: half whisper\n",
            "\n",
            "Found 3 document(s) (ranked by TF-IDF):\n",
            "  [2] Score: 0.0383, Link: https://www.gutenberg.org/files/98/98-0.txt)\n",
            "  [0] Score: 0.0309, Link: https://www.gutenberg.org/files/1342/1342-0.txt)\n",
            "  [1] Score: 0.0271, Link: https://www.gutenberg.org/files/11/11-0.txt)\n",
            "\n",
            "Search: boisterousness\n",
            "\n",
            "Found 1 document(s) (ranked by TF-IDF):\n",
            "  [0] Score: 0.0006, Link: https://www.gutenberg.org/files/1342/1342-0.txt)\n",
            "\n",
            "Search: Blazing strange\n",
            "\n",
            "Found 3 document(s) (ranked by TF-IDF):\n",
            "  [2] Score: 0.0159, Link: https://www.gutenberg.org/files/98/98-0.txt)\n",
            "  [1] Score: 0.0052, Link: https://www.gutenberg.org/files/11/11-0.txt)\n",
            "  [0] Score: 0.0038, Link: https://www.gutenberg.org/files/1342/1342-0.txt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sch6qFAT1Gwo"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}